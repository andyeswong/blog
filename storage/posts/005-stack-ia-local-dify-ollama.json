{
  "id": "005-stack-ia-local-dify-ollama",
  "title": "Mi Stack para Integrar IA Local en Aplicaciones Web: Dify + Ollama + Qwen3",
  "slug": "stack-ia-local-dify-ollama",
  "description": "Una guía práctica sobre cómo integrar asistentes de IA en aplicaciones web usando Dify selfhosted y Ollama con modelos locales, con ejemplos reales del blog que estás leyendo y el paradigma de tratar a la IA como una función programable.",
  "image_url": "/img/stack_ia.jpg",
  "tags": [
    "dify",
    "ollama",
    "qwen3",
    "ai integration",
    "selfhosted",
    "llm local",
    "api design",
    "javascript"
  ],
  "author": "AWONG",
  "reading_time": 15,
  "content": "<article><h1>Mi Stack para Integrar IA Local en Aplicaciones Web: Dify + Ollama + Qwen3</h1><p>Cuando hablamos de integrar IA en nuestras aplicaciones, muchos piensan inmediatamente en APIs de OpenAI o Claude con sus costos por token. Pero existe otra forma: correr modelos localmente con control total sobre tus datos y costos fijos. Hoy te muestro el stack que uso en este mismo blog que estás leyendo, con dos asistentes de IA funcionando en tiempo real.</p><h2>Los Asistentes de IA de Este Blog</h2><p>Si estás leyendo esto, ya tienes acceso a dos implementaciones reales de este stack. El primero es el sistema de recomendaciones en la página principal:</p><img src=\"/img/primer_ia.png\" alt=\"Sistema de recomendaciones de IA en la página principal\" /><p>Escribes qué quieres aprender o un tema específico, y el asistente te devuelve recomendaciones personalizadas de posts relevantes con enlaces directos. No es magia: es contexto estructurado.</p><p>El segundo asistente aparece cuando lees cualquier post en versión desktop:</p><img src=\"/img/segunda_ia.png\" alt=\"Asistente de IA para lectura de posts\" /><p>Este copiloto te permite hacer preguntas sobre el contenido, pedir resúmenes, solicitar ejemplos adicionales o profundizar en conceptos específicos. Tiene el contexto completo del artículo que estás leyendo.</p><h2>El Cambio de Paradigma: La IA como Función</h2><p>Aquí está el concepto clave que quiero que te lleves de este post: <strong>no siempre le tienes que mandar a la IA solamente el prompt del usuario</strong>. Hay que tratar a la IA como una función de programación que puede recibir diferentes parámetros.</p><p>Piénsalo así: cuando llamas una función en código, no le pasas solo el input del usuario. Le pasas configuración, contexto, datos de la base de datos, estado de la aplicación. Con los LLMs es exactamente igual. El prompt del usuario es solo uno de los parámetros que puedes enviar.</p><p>En mis implementaciones, envío estructuras JSON completas que incluyen la lista de posts disponibles (para recomendaciones), el contenido completo del post actual (para el asistente de lectura), instrucciones específicas de comportamiento, y el prompt del usuario como un campo más del objeto.</p><p>Esto transforma al LLM de un \"chatbot genérico\" a una función especializada con contexto rico.</p><h2>El Stack: Dify + Ollama + Qwen3</h2><h3>¿Por Qué Este Stack?</h3><p>Elegí estos componentes por razones específicas. <strong>Dify</strong> porque es una plataforma open-source que simplifica la orquestación de LLMs con una interfaz visual para diseñar agentes, manejo automático de conversaciones, y API lista para producción. <strong>Ollama</strong> porque permite correr modelos localmente con un solo comando, exponiendo una API compatible con OpenAI. <strong>Qwen3:14b</strong> porque ofrece un balance excelente entre capacidad y recursos, con soporte multilingüe nativo incluyendo español.</p><h3>Arquitectura General</h3><pre><code>┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n│   Frontend      │────▶│     Dify        │────▶│    Ollama       │\n│   (JavaScript)  │◀────│   (Orquestador) │◀────│   (Qwen3:14b)   │\n└─────────────────┘     └─────────────────┘     └─────────────────┘\n        │                        │                        │\n   Envía JSON              Gestiona                 Procesa\n   estructurado            conversación             inferencia</code></pre><h2>Quickstart: Instalando Ollama</h2><p>Ollama es sorprendentemente fácil de instalar. Descarga desde https://ollama.com/download para tu sistema operativo. En macOS puedes usar brew:</p><pre><code># macOS con Homebrew\nbrew install ollama\n\n# Linux\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Windows: descarga el instalador desde ollama.com</code></pre><p>Una vez instalado, inicia el servicio:</p><pre><code># Iniciar el servidor de Ollama\nollama serve</code></pre><p>Esto levanta una API en http://localhost:11434. Ahora descarga el modelo Qwen3:14b:</p><pre><code># Descargar Qwen3 14B (9.3GB)\nollama pull qwen3:14b\n\n# Verificar que se instaló\nollama list</code></pre><img src=\"/img/ollama_qwen.png\" alt=\"Ollama con modelo Qwen3 instalado\" /><p>Puedes probar el modelo directamente:</p><pre><code># Test rápido\nollama run qwen3:14b \"Explica qué es una API REST en una oración\"</code></pre><p>El modelo Qwen3:14b tiene 14.8 billones de parámetros con cuantización Q4_K_M, requiere aproximadamente 10GB de RAM/VRAM, y soporta un context window de 40K tokens. Para hardware más limitado, considera qwen3:8b (5.2GB) o qwen3:4b (2.5GB).</p><h2>Quickstart: Instalando Dify con Docker</h2><p>Dify se despliega mejor con Docker Compose. Primero clona el repositorio:</p><pre><code># Clonar Dify (usa la versión estable más reciente)\ngit clone https://github.com/langgenius/dify.git --branch 0.15.3\ncd dify/docker\n\n# Copiar archivo de configuración\ncp .env.example .env</code></pre><p>Inicia los contenedores:</p><pre><code># Iniciar todos los servicios\ndocker compose up -d\n\n# Verificar que todo está corriendo\ndocker compose ps</code></pre><p>Deberías ver 11 contenedores corriendo: api, worker, web, db (PostgreSQL), redis, nginx, weaviate, sandbox, y ssrf_proxy. Accede a http://localhost/install para crear tu cuenta de administrador.</p><h3>Conectando Dify con Ollama</h3><p>Aquí viene el paso crítico. Ve a Settings → Model Providers → Ollama y configura:</p><pre><code>Model Name: qwen3:14b\nBase URL: http://host.docker.internal:11434\nModel Type: LLM\nContext Length: 12000\nMax Tokens: 4096\nSupport for Vision: No</code></pre><p><strong>Importante:</strong> Si Dify corre en Docker y Ollama en tu host, usa <code>host.docker.internal</code> en lugar de <code>localhost</code>. En Linux, puede que necesites usar la IP de tu máquina o configurar la red de Docker.</p><p>Si obtienes errores de conexión, asegúrate de que Ollama esté escuchando en todas las interfaces. En Linux, edita el servicio:</p><pre><code># Editar configuración de Ollama\nsudo systemctl edit ollama.service\n\n# Agregar bajo [Service]:\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"</code></pre><h2>Creando el Agente en Dify</h2><p>Una vez conectado Ollama, crea una nueva aplicación tipo \"Chat\" en Dify. La configuración que uso tiene los siguientes parámetros clave:</p><pre><code>Tipo: Chat\nModelo: qwen3:14b (Ollama)\nContext Window: 12000 tokens\nTemperature: 0.7\nTop P: 0.95</code></pre><p>El context window de 12K tokens es suficiente para incluir posts completos más el historial de conversación. Ajusta según tus necesidades y la capacidad de tu hardware.</p><h2>Implementación: Sistema de Recomendaciones</h2><p>Veamos el código real que usa el blog para las recomendaciones. La clave está en cómo estructuro el JSON que envío a Dify:</p><pre><code>async function sendRecommendation({ query, posts, conversationId = null }) {\n  try {\n    // Crear contexto de posts con enlaces\n    const postsContext = posts.map(post => ({\n      title: post.title,\n      url: `/posts/${post.id}`,\n      description: post.description,\n      tags: post.tags\n    }));\n\n    // JSON estructurado con instrucción + datos + query\n    const contextData = {\n      instruction: \"Eres un asistente de blog que recomienda artículos. \" +\n        \"Basándote en la pregunta del usuario y la lista de posts disponibles, \" +\n        \"recomienda los posts más relevantes. Responde en español con markdown, \" +\n        \"incluyendo los enlaces a los posts recomendados usando el formato \" +\n        \"[Título](url). Explica brevemente por qué cada post es relevante.\",\n      available_posts: postsContext,\n      user_query: query\n    };\n\n    const body = {\n      inputs: {},\n      query: JSON.stringify(contextData),\n      response_mode: 'blocking',\n      user: 'blog-visitor',\n      conversation_id: conversationId || ''\n    };\n\n    const response = await fetch(`${DIFY_API_URL}/chat-messages`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${DIFY_API_KEY}`\n      },\n      body: JSON.stringify(body)\n    });\n\n    if (!response.ok) {\n      throw new Error(`Dify API error: ${response.status}`);\n    }\n\n    const data = await response.json();\n    return {\n      success: true,\n      answer: data.answer,\n      conversation_id: data.conversation_id,\n      message_id: data.message_id\n    };\n  } catch (error) {\n    console.error('Dify Recommendation API error:', error);\n    return { success: false, error: error.message };\n  }\n}</code></pre><p>Observa cómo el \"prompt\" real es un JSON stringificado que contiene las instrucciones del sistema, todos los posts disponibles, y la query del usuario como un campo más. El LLM recibe todo el contexto necesario para dar recomendaciones relevantes.</p><h2>Implementación: Asistente de Lectura de Posts</h2><p>Para el asistente que aparece mientras lees un post, la implementación es similar pero con contexto diferente:</p><pre><code>async function sendMessage({ query, post, conversationId = null }) {\n  try {\n    // Contexto: post completo + pregunta del usuario\n    const contextData = {\n      actual_post: post,\n      user_prompt: query\n    };\n\n    const body = {\n      inputs: {},\n      query: JSON.stringify(contextData),\n      response_mode: 'blocking',\n      user: 'blog-user',\n      conversation_id: conversationId || ''\n    };\n\n    const response = await fetch(`${DIFY_API_URL}/chat-messages`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${DIFY_API_KEY}`\n      },\n      body: JSON.stringify(body)\n    });\n\n    if (!response.ok) {\n      throw new Error(`Dify API error: ${response.status}`);\n    }\n\n    const data = await response.json();\n    return {\n      success: true,\n      answer: data.answer,\n      conversation_id: data.conversation_id,\n      message_id: data.message_id\n    };\n  } catch (error) {\n    console.error('Dify API error:', error);\n    return { success: false, error: error.message };\n  }\n}</code></pre><p>Y el system prompt en Dify para este agente:</p><pre><code>Eres un copiloto para el blog de Andres Wong.\n\nRecibirás un arreglo JSON con 2 llaves:\n{'actual_post': ..., 'user_prompt': ...}\n\nDebes asistir al usuario con sus preguntas acerca del actual_post.\nSi no recibes la estructura correcta del JSON, no podrás ayudar\ny deberás regresar un mensaje de error.\n\nTu respuesta tiene que ser siempre en formato markdown.</code></pre><p>Este system prompt establece el contrato: el agente espera un JSON específico y responde en markdown. Si la estructura no es correcta, falla explícitamente. Es programación defensiva aplicada a prompts.</p><h2>Más Ejemplos de IA como Función</h2><p>El paradigma de \"IA como función\" se extiende a muchos casos de uso. Aquí hay algunos ejemplos adicionales:</p><h3>Análisis de Código con Contexto</h3><pre><code>const contextData = {\n  instruction: \"Analiza el código y sugiere mejoras de performance\",\n  code: sourceCode,\n  language: \"javascript\",\n  framework: \"react\",\n  performance_metrics: currentMetrics,\n  user_question: userQuery\n};</code></pre><h3>Generador de Emails con Tono</h3><pre><code>const contextData = {\n  instruction: \"Genera un email profesional basado en los parámetros\",\n  recipient_info: {\n    name: \"María García\",\n    role: \"CTO\",\n    relationship: \"cliente potencial\"\n  },\n  email_purpose: \"seguimiento de demo\",\n  tone: \"formal pero amigable\",\n  key_points: [\"agradecer tiempo\", \"resumir beneficios\", \"proponer siguiente paso\"],\n  user_notes: userInput\n};</code></pre><h3>Asistente de Debugging</h3><pre><code>const contextData = {\n  instruction: \"Ayuda a debuggear el error basándote en el contexto\",\n  error_message: errorLog,\n  stack_trace: stackTrace,\n  relevant_code: codeSnippet,\n  environment: {\n    node_version: \"18.x\",\n    os: \"linux\",\n    dependencies: packageJson.dependencies\n  },\n  user_description: userQuery\n};</code></pre><p>En cada caso, el LLM recibe contexto estructurado que le permite dar respuestas específicas y útiles en lugar de respuestas genéricas.</p><h2>Consideraciones de Producción</h2><h3>Manejo del conversation_id</h3><p>Dify maneja automáticamente el historial de conversación a través del <code>conversation_id</code>. En la primera llamada lo envías vacío, y Dify te devuelve uno nuevo. En llamadas subsecuentes, envías el mismo ID para mantener el contexto.</p><pre><code>// Primera llamada: sin conversation_id\nconst firstResponse = await sendMessage({ query, post });\nconst convId = firstResponse.conversation_id;\n\n// Llamadas siguientes: incluir el ID\nconst followUp = await sendMessage({ \n  query: newQuery, \n  post, \n  conversationId: convId \n});</code></pre><h3>Streaming vs Blocking</h3><p>En los ejemplos uso <code>response_mode: 'blocking'</code> que espera la respuesta completa. Para mejor UX, considera usar streaming:</p><pre><code>const body = {\n  // ... resto igual\n  response_mode: 'streaming'\n};\n\nconst response = await fetch(url, options);\nconst reader = response.body.getReader();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  // Procesar chunks incrementalmente\n  const text = new TextDecoder().decode(value);\n  updateUI(text);\n}</code></pre><h3>Límites de Contexto</h3><p>Con 12K tokens de contexto, puedo incluir posts de hasta ~8000 palabras más historial de conversación. Para posts más largos, considera truncar el contenido o usar técnicas de chunking con embeddings.</p><h2>Costos y Rendimiento</h2><p>Una de las grandes ventajas de este stack es el costo predecible. Una vez que tienes el hardware corriendo Ollama, el costo marginal por request es esencialmente cero: solo electricidad.</p><p>En mi setup (RTX 3080 10GB), Qwen3:14b genera aproximadamente 30-40 tokens por segundo. Para una respuesta típica de 200 tokens, son ~5 segundos. No es tan rápido como las APIs cloud, pero es consistente y sin límites de rate.</p><p>Si tu hardware es más limitado, considera usar qwen3:8b (5.2GB), qwen3:4b (2.5GB), o correr en CPU (más lento pero funcional).</p><h2>Conclusión</h2><p>El stack Dify + Ollama + Qwen3 te da control total sobre tus integraciones de IA con costos predecibles y privacidad de datos. Pero el verdadero insight es el paradigma: trata a la IA como una función que recibe parámetros estructurados, no solo como un chatbot que recibe texto plano.</p><p>Cuando envías contexto rico (posts disponibles, contenido actual, métricas, configuración) junto con el prompt del usuario, transformas un LLM genérico en una herramienta especializada para tu caso de uso específico.</p><p>Los dos asistentes de este blog son prueba de concepto: el mismo modelo Qwen3:14b, con diferentes contextos JSON, se comporta como un recomendador de contenido o como un tutor de lectura. La diferencia está en los parámetros que le pasas, exactamente como una función bien diseñada.</p></article>",
  "metadata": {
    "created_time": "2025-11-20T10:00:00Z",
    "modification_time": "2025-11-24T19:25:57.683Z",
    "version": "1.0",
    "status": "published",
    "seo_keywords": "dify selfhosted, ollama local llm, qwen3 14b, ai integration javascript, llm as function, dify ollama setup, local ai assistant, structured prompts, context-aware ai, ai api design, docker compose dify, blog ai assistant"
  },
  "featured": true,
  "views": 31
}